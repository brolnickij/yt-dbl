# ── API keys ─────────────────────────────────────────────────────────────
# Anthropic API key (REQUIRED for translation step)
YT_DBL_ANTHROPIC_API_KEY=
# HuggingFace token (optional, only needed for gated models)
# YT_DBL_HF_TOKEN=

# ── Pipeline ─────────────────────────────────────────────────────────────
# Target dubbing language (ISO 639-1 code)
# Supported TTS: ru, en, de, fr, es, it, pt, zh, ja, ko, ar, hi, tr, nl, pl, uk
# YT_DBL_TARGET_LANGUAGE=ru

# Output container format: mp4 | mkv
# YT_DBL_OUTPUT_FORMAT=mp4

# Subtitle mode: softsub (muxed track) | hardsub (burned in) | none
# YT_DBL_SUBTITLE_MODE=softsub

# ── Audio ────────────────────────────────────────────────────────────────
# Background music volume during speech (0.0 = mute, 1.0 = full)
# YT_DBL_BACKGROUND_VOLUME=0.15

# Reduce background when speech is active (sidechain ducking)
# YT_DBL_BACKGROUND_DUCKING=true

# Maximum TTS speed-up factor (1.0 = no speed-up, 2.0 = double speed)
# If dubbed segment is longer than original, it is sped up to fit.
# YT_DBL_MAX_SPEED_FACTOR=1.4

# Duration (sec) of the voice reference clip used for voice cloning
# YT_DBL_VOICE_REF_DURATION=7.0

# Audio sample rate for download & final output (Hz)
# YT_DBL_SAMPLE_RATE=48000

# ── Separation (BS-RoFormer) ────────────────────────────────────────────
# Separation model checkpoint filename
# YT_DBL_SEPARATION_MODEL=model_bs_roformer_ep_317_sdr_12.9755.ckpt

# Segment size for chunked separation (larger = more VRAM, better quality)
# YT_DBL_SEPARATION_SEGMENT_SIZE=256

# Overlap between separation segments (higher = smoother joins)
# YT_DBL_SEPARATION_OVERLAP=8

# Batch size for separation (0 = auto-detect by RAM)
# NB: batch_size is ignored by audio-separator for Roformer models.
# YT_DBL_SEPARATION_BATCH_SIZE=0

# FP16 mixed precision (~1.5-2x faster on Apple Silicon MPS)
# YT_DBL_SEPARATION_USE_AUTOCAST=true

# ── Transcription (VibeVoice-ASR) ───────────────────────────────────────
# ASR + diarization model (HuggingFace repo ID)
# YT_DBL_TRANSCRIPTION_ASR_MODEL=mlx-community/VibeVoice-ASR-4bit

# Word-level alignment model
# YT_DBL_TRANSCRIPTION_ALIGNER_MODEL=mlx-community/Qwen3-ForcedAligner-0.6B-8bit

# ASR sampling temperature (0.0 = greedy)
# YT_DBL_TRANSCRIPTION_TEMPERATURE=0.0

# Max duration per ASR chunk (minutes).
# Audio longer than this is split into overlapping chunks automatically.
# YT_DBL_TRANSCRIPTION_MAX_CHUNK_MINUTES=30.0

# Overlap between ASR chunks (for speaker reconciliation across boundaries)
# YT_DBL_TRANSCRIPTION_CHUNK_OVERLAP_MINUTES=2.0

# ── Synthesis (Qwen3-TTS) ───────────────────────────────────────────────
# TTS model with voice cloning
# YT_DBL_TTS_MODEL=mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16

# TTS sampling temperature (higher = more expressive, less stable)
# YT_DBL_TTS_TEMPERATURE=0.9

# Top-K sampling
# YT_DBL_TTS_TOP_K=50

# Top-P (nucleus) sampling
# YT_DBL_TTS_TOP_P=1.0

# Repetition penalty (prevents loops)
# YT_DBL_TTS_REPETITION_PENALTY=1.05

# TTS native sample rate (do not change unless you swap models)
# YT_DBL_TTS_SAMPLE_RATE=24000

# ── Translation (Claude) ────────────────────────────────────────────────
# Anthropic model for translation
# YT_DBL_CLAUDE_MODEL=claude-sonnet-4-5

# Max segments per API call. For long audio (>1h, ~2000+ segments)
# the pipeline splits into batches to stay within output token limits.
# YT_DBL_TRANSLATION_BATCH_SIZE=300

# Max output tokens per translation API call (Claude Sonnet 4.5 limit: 32768)
# YT_DBL_TRANSLATION_MAX_TOKENS=32768

# ── Models & paths ───────────────────────────────────────────────────────
# Max ML models kept in memory simultaneously (0 = auto-detect by RAM)
# RAM <= 16 GB → 1  |  17-31 GB → 2  |  32+ GB → 3
# YT_DBL_MAX_LOADED_MODELS=0

# Working directory for all jobs and outputs
# YT_DBL_WORK_DIR=dubbed

# HuggingFace model cache directory
# YT_DBL_MODEL_CACHE_DIR=~/.cache/yt-dbl/models

# Path to ffmpeg binary (empty = auto-detect, prefers ffmpeg-full)
# YT_DBL_FFMPEG_PATH=
